{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb5f076-4d28-4653-8699-f18b3483dbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\r-cet\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\r-cet\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\r-cet\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\r-cet\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\r-cet\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\r-cet\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\r-cet\\anaconda3\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\r-cet\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\r-cet\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9241f3b5-87b1-4be6-af10-53d24edd91fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data keys: dict_keys(['model'])\n",
      "Key: model, Type: <class 'dict'>, Shape: Not a Tensor\n",
      "Key: model, Value: {'image_bind_proj.weight': tensor([[-0.0481,  0.0905, -0.0778,  ...,  0.0458, -0.0254, -0.0264],\n",
      "        [-0.0886, -0.0605, -0.0037,  ...,  0.0088,  0.0038, -0.0023],\n",
      "        [ 0.0332,  0.0343, -0.0706,  ..., -0.0107,  0.0355,  0.0471],\n",
      "        ...,\n",
      "        [ 0.0232,  0.0233, -0.0288,  ..., -0.0446, -0.0059,  0.0131],\n",
      "        [-0.0684, -0.0323, -0.0221,  ..., -0.0434,  0.0142, -0.0822],\n",
      "        [ 0.0202, -0.0119, -0.0082,  ...,  0.0540, -0.0152,  0.0180]]), 'image_bind_proj.bias': tensor([ 1.0962e-02,  2.5276e-02, -1.7578e-03,  ..., -5.1113e-05,\n",
      "        -2.6642e-02, -5.3781e-03]), 'image_bind_norm_1.weight': tensor([0.3981, 0.5456, 0.5144,  ..., 0.4558, 0.4011, 0.4011]), 'image_bind_f1_1.weight': tensor([[ 0.0323,  0.0602, -0.0234,  ...,  0.1253,  0.0596, -0.0030],\n",
      "        [-0.0022, -0.0368,  0.0547,  ..., -0.0194, -0.0461, -0.0601],\n",
      "        [ 0.0295, -0.0098, -0.0991,  ...,  0.0157, -0.0556, -0.0571],\n",
      "        ...,\n",
      "        [ 0.0170, -0.0080, -0.0052,  ..., -0.0416,  0.0344, -0.0309],\n",
      "        [ 0.0044, -0.0549,  0.0237,  ...,  0.0477, -0.0094, -0.0023],\n",
      "        [ 0.0069, -0.0780,  0.0975,  ..., -0.0022,  0.0155,  0.0091]]), 'image_bind_f2_1.weight': tensor([[-0.0237,  0.0344, -0.0007,  ...,  0.0921,  0.0654, -0.0435],\n",
      "        [-0.1031,  0.0924,  0.0164,  ...,  0.0167, -0.0056, -0.0394],\n",
      "        [ 0.0008,  0.0616, -0.0144,  ...,  0.0029,  0.0366, -0.0014],\n",
      "        ...,\n",
      "        [ 0.0598,  0.0586, -0.0360,  ..., -0.0165,  0.0002,  0.0080],\n",
      "        [ 0.0419, -0.0172, -0.0461,  ...,  0.0440,  0.0597, -0.0269],\n",
      "        [-0.0049,  0.0112,  0.0352,  ..., -0.0239,  0.0609, -0.0010]]), 'image_bind_f3_1.weight': tensor([[ 0.0375, -0.0100,  0.0209,  ...,  0.0619, -0.0041, -0.0356],\n",
      "        [ 0.0211, -0.1051, -0.0142,  ..., -0.0360, -0.0667,  0.0218],\n",
      "        [-0.0019,  0.0369, -0.0369,  ...,  0.0189,  0.0912,  0.0238],\n",
      "        ...,\n",
      "        [-0.0811, -0.0827,  0.0379,  ...,  0.0097,  0.0069,  0.0022],\n",
      "        [ 0.0454, -0.0805, -0.0167,  ...,  0.0183, -0.0373, -0.0416],\n",
      "        [-0.0373,  0.0035, -0.0430,  ...,  0.0234, -0.0087,  0.0212]]), 'image_bind_norm_2.weight': tensor([0.4228, 0.2961, 0.3562,  ..., 0.4389, 0.4395, 0.5103]), 'image_bind_f1_2.weight': tensor([[-0.0032, -0.0419, -0.0665,  ...,  0.0566,  0.0008,  0.0274],\n",
      "        [ 0.1394, -0.0016,  0.0788,  ..., -0.0580,  0.0219, -0.0111],\n",
      "        [-0.0682, -0.0173, -0.0185,  ..., -0.0286,  0.0341, -0.0090],\n",
      "        ...,\n",
      "        [ 0.0082, -0.1209,  0.0071,  ..., -0.0851,  0.0371,  0.0124],\n",
      "        [ 0.1210,  0.0060, -0.0284,  ..., -0.0556,  0.0056, -0.0955],\n",
      "        [ 0.0025,  0.0080, -0.0146,  ...,  0.0142,  0.0096, -0.0504]]), 'image_bind_f2_2.weight': tensor([[-2.2505e-02,  6.6927e-02, -2.5174e-02,  ..., -1.2039e-02,\n",
      "          8.9333e-02,  1.6627e-02],\n",
      "        [ 1.9738e-02, -5.0764e-02,  2.3605e-02,  ..., -4.2541e-02,\n",
      "          1.4946e-02,  1.9993e-02],\n",
      "        [ 3.7404e-02,  3.2628e-02,  1.6121e-02,  ..., -3.9914e-02,\n",
      "         -2.3496e-02,  8.4632e-03],\n",
      "        ...,\n",
      "        [ 1.1902e-02, -3.6434e-02, -3.6280e-02,  ...,  2.5043e-02,\n",
      "         -3.0282e-02, -3.9640e-02],\n",
      "        [ 3.7569e-02, -4.7966e-02,  6.9989e-05,  ...,  9.9723e-03,\n",
      "         -3.4924e-02, -5.3099e-03],\n",
      "        [ 2.1135e-02, -1.3371e-02,  5.6210e-02,  ..., -4.8291e-03,\n",
      "         -1.6357e-02, -3.6130e-02]]), 'image_bind_f3_2.weight': tensor([[-0.0252, -0.0319, -0.0109,  ..., -0.0545,  0.0378,  0.0477],\n",
      "        [ 0.0115,  0.0183, -0.0329,  ..., -0.0131, -0.0679, -0.0203],\n",
      "        [-0.0318,  0.0269, -0.0162,  ..., -0.0280,  0.0143,  0.0248],\n",
      "        ...,\n",
      "        [ 0.0455, -0.0460, -0.0776,  ...,  0.0213,  0.0815,  0.0141],\n",
      "        [ 0.0628, -0.0040, -0.0032,  ...,  0.0176,  0.0354, -0.0197],\n",
      "        [-0.0022, -0.0211, -0.0028,  ...,  0.0597, -0.0049,  0.0472]]), 'image_bind_norm_3.weight': tensor([0.3237, 0.3586, 0.3846,  ..., 0.4020, 0.4053, 0.3824]), 'image_bind_f1_3.weight': tensor([[-0.0052,  0.0421,  0.0445,  ...,  0.0250, -0.0650,  0.0183],\n",
      "        [-0.0385,  0.0531,  0.1093,  ..., -0.0366, -0.0330,  0.0234],\n",
      "        [-0.0290, -0.0270,  0.0340,  ..., -0.0150,  0.0322,  0.0026],\n",
      "        ...,\n",
      "        [ 0.0365,  0.0859,  0.0538,  ...,  0.0089, -0.0074, -0.0124],\n",
      "        [ 0.0291,  0.0087, -0.0687,  ..., -0.0170,  0.0277, -0.0517],\n",
      "        [-0.0044,  0.0069,  0.0300,  ...,  0.0304,  0.0759,  0.0441]]), 'image_bind_f2_3.weight': tensor([[ 0.0446,  0.0260, -0.0203,  ..., -0.0209, -0.0546,  0.0041],\n",
      "        [ 0.0107, -0.0358, -0.0006,  ...,  0.0133,  0.0038,  0.0008],\n",
      "        [-0.0856,  0.0866, -0.0269,  ..., -0.0058,  0.0166, -0.0295],\n",
      "        ...,\n",
      "        [-0.0117, -0.0212,  0.0655,  ...,  0.0431,  0.0217,  0.0114],\n",
      "        [ 0.0451,  0.0665,  0.0883,  ..., -0.0200,  0.0146,  0.0111],\n",
      "        [-0.0120,  0.0025, -0.0754,  ...,  0.0118, -0.0249,  0.0515]]), 'image_bind_f3_3.weight': tensor([[-0.0140, -0.0060, -0.0690,  ...,  0.0385,  0.0161,  0.0589],\n",
      "        [-0.0551,  0.0104, -0.0303,  ..., -0.0235, -0.0765,  0.0295],\n",
      "        [-0.0178,  0.0262,  0.0135,  ..., -0.0097,  0.0380, -0.0311],\n",
      "        ...,\n",
      "        [-0.0025, -0.0075, -0.0322,  ..., -0.0257, -0.0535,  0.0460],\n",
      "        [ 0.0079, -0.0364,  0.0145,  ...,  0.0090,  0.0177, -0.0226],\n",
      "        [ 0.0002,  0.0037,  0.0653,  ...,  0.0602,  0.0410,  0.1014]]), 'llama.layers.0.attention.gate': tensor([[[[-0.0007]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[ 0.0008]]]]), 'llama.layers.0.attention_norm.weight': tensor([0.0432, 0.0181, 0.0153,  ..., 0.0276, 0.0209, 0.0249],\n",
      "       dtype=torch.float16), 'llama.layers.0.ffn_norm.weight': tensor([0.0601, 0.0830, 0.0859,  ..., 0.0693, 0.0850, 0.0771],\n",
      "       dtype=torch.float16), 'llama.layers.1.attention.gate': tensor([[[[-2.5933e-03]],\n",
      "\n",
      "         [[-2.9948e-03]],\n",
      "\n",
      "         [[-9.3843e-04]],\n",
      "\n",
      "         [[-1.5986e-03]],\n",
      "\n",
      "         [[-5.3743e-03]],\n",
      "\n",
      "         [[-2.6694e-03]],\n",
      "\n",
      "         [[ 2.2305e-03]],\n",
      "\n",
      "         [[-3.0208e-04]],\n",
      "\n",
      "         [[-3.0966e-04]],\n",
      "\n",
      "         [[-1.1324e-04]],\n",
      "\n",
      "         [[-2.6028e-03]],\n",
      "\n",
      "         [[-5.1183e-04]],\n",
      "\n",
      "         [[-3.5580e-03]],\n",
      "\n",
      "         [[-2.0876e-03]],\n",
      "\n",
      "         [[ 5.2647e-05]],\n",
      "\n",
      "         [[ 2.1075e-03]],\n",
      "\n",
      "         [[ 1.7469e-03]],\n",
      "\n",
      "         [[ 2.1604e-03]],\n",
      "\n",
      "         [[ 9.6817e-04]],\n",
      "\n",
      "         [[ 4.5421e-04]],\n",
      "\n",
      "         [[-5.0043e-04]],\n",
      "\n",
      "         [[ 1.7986e-03]],\n",
      "\n",
      "         [[-1.4792e-03]],\n",
      "\n",
      "         [[-2.0556e-03]],\n",
      "\n",
      "         [[ 1.3396e-03]],\n",
      "\n",
      "         [[-1.0731e-03]],\n",
      "\n",
      "         [[-8.6835e-05]],\n",
      "\n",
      "         [[-2.1637e-03]],\n",
      "\n",
      "         [[ 5.6355e-04]],\n",
      "\n",
      "         [[-1.5984e-03]],\n",
      "\n",
      "         [[ 1.9388e-03]],\n",
      "\n",
      "         [[ 1.7264e-04]]]]), 'llama.layers.1.attention_norm.weight': tensor([0.1523, 0.0732, 0.0815,  ..., 0.0649, 0.0747, 0.0854],\n",
      "       dtype=torch.float16), 'llama.layers.1.ffn_norm.weight': tensor([0.0972, 0.1089, 0.1143,  ..., 0.1055, 0.1084, 0.1089],\n",
      "       dtype=torch.float16), 'llama.layers.2.attention.gate': tensor([[[[-0.0047]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[ 0.0008]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0047]]]]), 'llama.layers.2.attention_norm.weight': tensor([0.1426, 0.1250, 0.1406,  ..., 0.1270, 0.1187, 0.1455],\n",
      "       dtype=torch.float16), 'llama.layers.2.ffn_norm.weight': tensor([0.1387, 0.1416, 0.1387,  ..., 0.1338, 0.1396, 0.1377],\n",
      "       dtype=torch.float16), 'llama.layers.3.attention.gate': tensor([[[[ 2.3653e-03]],\n",
      "\n",
      "         [[ 4.5919e-04]],\n",
      "\n",
      "         [[ 1.4272e-03]],\n",
      "\n",
      "         [[ 1.2839e-04]],\n",
      "\n",
      "         [[-2.5419e-03]],\n",
      "\n",
      "         [[ 1.3803e-03]],\n",
      "\n",
      "         [[ 3.5936e-03]],\n",
      "\n",
      "         [[-4.4050e-03]],\n",
      "\n",
      "         [[-3.3319e-03]],\n",
      "\n",
      "         [[-2.5451e-03]],\n",
      "\n",
      "         [[-3.3099e-03]],\n",
      "\n",
      "         [[ 3.1018e-03]],\n",
      "\n",
      "         [[ 2.7757e-03]],\n",
      "\n",
      "         [[ 3.2573e-03]],\n",
      "\n",
      "         [[-3.1933e-04]],\n",
      "\n",
      "         [[-2.2086e-03]],\n",
      "\n",
      "         [[ 1.4515e-03]],\n",
      "\n",
      "         [[ 1.7213e-03]],\n",
      "\n",
      "         [[-2.6111e-03]],\n",
      "\n",
      "         [[ 2.2520e-03]],\n",
      "\n",
      "         [[-5.0176e-04]],\n",
      "\n",
      "         [[ 4.6606e-04]],\n",
      "\n",
      "         [[ 8.5978e-04]],\n",
      "\n",
      "         [[ 2.3519e-03]],\n",
      "\n",
      "         [[ 3.3715e-03]],\n",
      "\n",
      "         [[ 1.2461e-04]],\n",
      "\n",
      "         [[-9.0116e-04]],\n",
      "\n",
      "         [[ 6.7662e-04]],\n",
      "\n",
      "         [[-3.2885e-03]],\n",
      "\n",
      "         [[ 2.6167e-05]],\n",
      "\n",
      "         [[-2.6965e-03]],\n",
      "\n",
      "         [[-2.9464e-03]]]]), 'llama.layers.3.attention_norm.weight': tensor([0.2080, 0.2061, 0.2217,  ..., 0.2051, 0.2109, 0.2100],\n",
      "       dtype=torch.float16), 'llama.layers.3.ffn_norm.weight': tensor([0.1699, 0.1758, 0.1670,  ..., 0.1699, 0.1719, 0.1670],\n",
      "       dtype=torch.float16), 'llama.layers.4.attention.gate': tensor([[[[-0.0012]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[ 0.0005]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[ 0.0023]]]]), 'llama.layers.4.attention_norm.weight': tensor([0.2314, 0.2363, 0.2344,  ..., 0.2500, 0.2314, 0.2500],\n",
      "       dtype=torch.float16), 'llama.layers.4.ffn_norm.weight': tensor([0.1846, 0.1826, 0.1836,  ..., 0.1816, 0.1904, 0.1777],\n",
      "       dtype=torch.float16), 'llama.layers.5.attention.gate': tensor([[[[-0.0062]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[-0.0001]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[ 0.0085]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0066]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[-0.0051]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.0030]]]]), 'llama.layers.5.attention_norm.weight': tensor([0.2852, 0.2812, 0.2832,  ..., 0.2871, 0.2734, 0.2852],\n",
      "       dtype=torch.float16), 'llama.layers.5.ffn_norm.weight': tensor([0.1953, 0.1973, 0.1943,  ..., 0.1934, 0.2021, 0.1865],\n",
      "       dtype=torch.float16), 'llama.layers.6.attention.gate': tensor([[[[-0.0062]],\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         [[-0.0051]],\n",
      "\n",
      "         [[-0.0070]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[-0.0079]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[ 0.0094]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[-0.0062]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[-0.0028]]]]), 'llama.layers.6.attention_norm.weight': tensor([0.2891, 0.2793, 0.2871,  ..., 0.2871, 0.2832, 0.2871],\n",
      "       dtype=torch.float16), 'llama.layers.6.ffn_norm.weight': tensor([0.2041, 0.2041, 0.1992,  ..., 0.2012, 0.2100, 0.1924],\n",
      "       dtype=torch.float16), 'llama.layers.7.attention.gate': tensor([[[[ 0.0048]],\n",
      "\n",
      "         [[-0.0071]],\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         [[-0.0201]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0056]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[-0.0061]],\n",
      "\n",
      "         [[ 0.0063]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[-0.0127]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         [[ 0.0064]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[-0.0076]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0060]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[ 0.0065]]]]), 'llama.layers.7.attention_norm.weight': tensor([0.2969, 0.2852, 0.2852,  ..., 0.2969, 0.2852, 0.2969],\n",
      "       dtype=torch.float16), 'llama.layers.7.ffn_norm.weight': tensor([0.2109, 0.2119, 0.2051,  ..., 0.2070, 0.2148, 0.1982],\n",
      "       dtype=torch.float16), 'llama.layers.8.attention.gate': tensor([[[[ 0.0098]],\n",
      "\n",
      "         [[-0.0169]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[ 0.0071]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[-0.0051]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.0100]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[ 0.0067]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0086]],\n",
      "\n",
      "         [[ 0.0136]],\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0063]]]]), 'llama.layers.8.attention_norm.weight': tensor([0.3066, 0.3047, 0.3066,  ..., 0.3164, 0.3223, 0.3125],\n",
      "       dtype=torch.float16), 'llama.layers.8.ffn_norm.weight': tensor([0.2119, 0.2119, 0.2090,  ..., 0.2090, 0.2197, 0.1982],\n",
      "       dtype=torch.float16), 'llama.layers.9.attention.gate': tensor([[[[ 0.0144]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[ 0.0098]],\n",
      "\n",
      "         [[-0.0110]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[ 0.0053]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[ 0.0109]],\n",
      "\n",
      "         [[ 0.0119]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[ 0.0093]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         [[ 0.0056]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         [[ 0.0053]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         [[ 0.0064]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         [[-0.0132]],\n",
      "\n",
      "         [[ 0.0118]],\n",
      "\n",
      "         [[ 0.0106]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0048]]]]), 'llama.layers.9.attention_norm.weight': tensor([0.3223, 0.3262, 0.3184,  ..., 0.3418, 0.3203, 0.3262],\n",
      "       dtype=torch.float16), 'llama.layers.9.ffn_norm.weight': tensor([0.2109, 0.2129, 0.2168,  ..., 0.2090, 0.2207, 0.1973],\n",
      "       dtype=torch.float16), 'llama.layers.10.attention.gate': tensor([[[[ 0.0033]],\n",
      "\n",
      "         [[ 0.0081]],\n",
      "\n",
      "         [[ 0.0082]],\n",
      "\n",
      "         [[-0.0120]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0133]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0112]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0144]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         [[ 0.0056]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0062]],\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         [[ 0.0114]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0001]],\n",
      "\n",
      "         [[-0.0087]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[ 0.0027]]]]), 'llama.layers.10.attention_norm.weight': tensor([0.3457, 0.3379, 0.3359,  ..., 0.3359, 0.3379, 0.3281],\n",
      "       dtype=torch.float16), 'llama.layers.10.ffn_norm.weight': tensor([0.2207, 0.2188, 0.2266,  ..., 0.2207, 0.2275, 0.2070],\n",
      "       dtype=torch.float16), 'llama.layers.11.attention.gate': tensor([[[[ 8.9918e-05]],\n",
      "\n",
      "         [[ 3.6514e-04]],\n",
      "\n",
      "         [[-1.7525e-02]],\n",
      "\n",
      "         [[ 1.1032e-02]],\n",
      "\n",
      "         [[ 1.0463e-02]],\n",
      "\n",
      "         [[-2.5146e-03]],\n",
      "\n",
      "         [[ 1.1177e-02]],\n",
      "\n",
      "         [[-4.4388e-03]],\n",
      "\n",
      "         [[-1.0457e-03]],\n",
      "\n",
      "         [[ 1.8296e-02]],\n",
      "\n",
      "         [[-4.9072e-05]],\n",
      "\n",
      "         [[ 3.0403e-02]],\n",
      "\n",
      "         [[-7.7720e-03]],\n",
      "\n",
      "         [[ 1.0537e-03]],\n",
      "\n",
      "         [[ 2.0246e-04]],\n",
      "\n",
      "         [[-1.7459e-03]],\n",
      "\n",
      "         [[ 7.0181e-03]],\n",
      "\n",
      "         [[ 6.2719e-03]],\n",
      "\n",
      "         [[ 2.7851e-03]],\n",
      "\n",
      "         [[-3.5918e-03]],\n",
      "\n",
      "         [[ 9.3618e-03]],\n",
      "\n",
      "         [[-1.2737e-02]],\n",
      "\n",
      "         [[ 7.8351e-03]],\n",
      "\n",
      "         [[-2.1052e-03]],\n",
      "\n",
      "         [[ 5.8583e-03]],\n",
      "\n",
      "         [[ 2.8785e-03]],\n",
      "\n",
      "         [[ 1.0908e-02]],\n",
      "\n",
      "         [[ 7.4246e-03]],\n",
      "\n",
      "         [[ 1.3873e-02]],\n",
      "\n",
      "         [[-3.7709e-03]],\n",
      "\n",
      "         [[ 1.5106e-02]],\n",
      "\n",
      "         [[-1.7223e-03]]]]), 'llama.layers.11.attention_norm.weight': tensor([0.3105, 0.2949, 0.3105,  ..., 0.3125, 0.3164, 0.2988],\n",
      "       dtype=torch.float16), 'llama.layers.11.ffn_norm.weight': tensor([0.2246, 0.2217, 0.2266,  ..., 0.2227, 0.2324, 0.2090],\n",
      "       dtype=torch.float16), 'llama.layers.12.attention.gate': tensor([[[[-0.0065]],\n",
      "\n",
      "         [[ 0.0167]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[-0.0148]],\n",
      "\n",
      "         [[ 0.0105]],\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0108]],\n",
      "\n",
      "         [[ 0.0155]],\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0099]],\n",
      "\n",
      "         [[ 0.0120]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[ 0.0082]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0099]],\n",
      "\n",
      "         [[ 0.0078]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[ 0.0082]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         [[-0.0037]],\n",
      "\n",
      "         [[ 0.0079]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         [[ 0.0115]],\n",
      "\n",
      "         [[ 0.0116]],\n",
      "\n",
      "         [[ 0.0126]]]]), 'llama.layers.12.attention_norm.weight': tensor([0.3652, 0.3652, 0.3516,  ..., 0.3691, 0.3633, 0.3574],\n",
      "       dtype=torch.float16), 'llama.layers.12.ffn_norm.weight': tensor([0.2363, 0.2393, 0.2373,  ..., 0.2314, 0.2412, 0.2197],\n",
      "       dtype=torch.float16), 'llama.layers.13.attention.gate': tensor([[[[ 0.0300]],\n",
      "\n",
      "         [[ 0.0058]],\n",
      "\n",
      "         [[-0.0172]],\n",
      "\n",
      "         [[ 0.0096]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0117]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         [[ 0.0120]],\n",
      "\n",
      "         [[-0.0136]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[ 0.0116]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[ 0.0172]],\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[-0.0043]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[-0.0146]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[-0.0118]],\n",
      "\n",
      "         [[ 0.0104]],\n",
      "\n",
      "         [[ 0.0091]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[ 0.0088]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0134]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[-0.0030]]]]), 'llama.layers.13.attention_norm.weight': tensor([0.3770, 0.3809, 0.3750,  ..., 0.3828, 0.3867, 0.3711],\n",
      "       dtype=torch.float16), 'llama.layers.13.ffn_norm.weight': tensor([0.2451, 0.2432, 0.2461,  ..., 0.2412, 0.2500, 0.2295],\n",
      "       dtype=torch.float16), 'llama.layers.14.attention.gate': tensor([[[[-0.0050]],\n",
      "\n",
      "         [[-0.0070]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[-0.0109]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[ 0.0177]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[ 0.0129]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0169]],\n",
      "\n",
      "         [[-0.0104]],\n",
      "\n",
      "         [[-0.0115]],\n",
      "\n",
      "         [[-0.0270]],\n",
      "\n",
      "         [[-0.0100]],\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[-0.0115]],\n",
      "\n",
      "         [[-0.0137]],\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         [[ 0.0163]],\n",
      "\n",
      "         [[ 0.0008]],\n",
      "\n",
      "         [[ 0.0103]],\n",
      "\n",
      "         [[ 0.0153]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0091]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[ 0.0058]]]]), 'llama.layers.14.attention_norm.weight': tensor([0.3633, 0.3652, 0.3613,  ..., 0.3828, 0.3770, 0.3613],\n",
      "       dtype=torch.float16), 'llama.layers.14.ffn_norm.weight': tensor([0.2578, 0.2559, 0.2578,  ..., 0.2520, 0.2617, 0.2441],\n",
      "       dtype=torch.float16), 'llama.layers.15.attention.gate': tensor([[[[ 0.0073]],\n",
      "\n",
      "         [[-0.0095]],\n",
      "\n",
      "         [[ 0.0151]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0123]],\n",
      "\n",
      "         [[ 0.0167]],\n",
      "\n",
      "         [[-0.0087]],\n",
      "\n",
      "         [[-0.0117]],\n",
      "\n",
      "         [[ 0.0156]],\n",
      "\n",
      "         [[-0.0037]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[ 0.0145]],\n",
      "\n",
      "         [[ 0.0154]],\n",
      "\n",
      "         [[ 0.0084]],\n",
      "\n",
      "         [[ 0.0120]],\n",
      "\n",
      "         [[-0.0160]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[ 0.0071]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[-0.0091]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[ 0.0099]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[-0.0122]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0141]]]]), 'llama.layers.15.attention_norm.weight': tensor([0.3652, 0.3711, 0.3594,  ..., 0.3809, 0.3809, 0.3750],\n",
      "       dtype=torch.float16), 'llama.layers.15.ffn_norm.weight': tensor([0.2637, 0.2676, 0.2656,  ..., 0.2617, 0.2715, 0.2559],\n",
      "       dtype=torch.float16), 'llama.layers.16.attention.gate': tensor([[[[ 1.3781e-02]],\n",
      "\n",
      "         [[ 1.0518e-03]],\n",
      "\n",
      "         [[-7.8544e-04]],\n",
      "\n",
      "         [[-7.0995e-03]],\n",
      "\n",
      "         [[-4.0008e-03]],\n",
      "\n",
      "         [[ 1.0031e-02]],\n",
      "\n",
      "         [[ 7.9682e-03]],\n",
      "\n",
      "         [[ 1.1626e-03]],\n",
      "\n",
      "         [[-4.0928e-03]],\n",
      "\n",
      "         [[-6.3312e-03]],\n",
      "\n",
      "         [[ 3.5103e-03]],\n",
      "\n",
      "         [[-4.6981e-03]],\n",
      "\n",
      "         [[ 1.2858e-02]],\n",
      "\n",
      "         [[ 3.3747e-03]],\n",
      "\n",
      "         [[-4.6662e-03]],\n",
      "\n",
      "         [[ 1.5123e-02]],\n",
      "\n",
      "         [[-1.3601e-03]],\n",
      "\n",
      "         [[-6.4633e-03]],\n",
      "\n",
      "         [[-8.2500e-05]],\n",
      "\n",
      "         [[-4.2386e-03]],\n",
      "\n",
      "         [[ 2.4445e-03]],\n",
      "\n",
      "         [[-4.7048e-03]],\n",
      "\n",
      "         [[-2.0889e-03]],\n",
      "\n",
      "         [[ 3.3774e-03]],\n",
      "\n",
      "         [[-7.0208e-03]],\n",
      "\n",
      "         [[ 5.9096e-03]],\n",
      "\n",
      "         [[ 1.9373e-04]],\n",
      "\n",
      "         [[ 1.1788e-02]],\n",
      "\n",
      "         [[-1.2150e-05]],\n",
      "\n",
      "         [[ 8.2743e-03]],\n",
      "\n",
      "         [[ 5.1240e-03]],\n",
      "\n",
      "         [[-7.0067e-03]]]]), 'llama.layers.16.attention_norm.weight': tensor([0.3652, 0.3594, 0.3555,  ..., 0.3730, 0.3789, 0.3730],\n",
      "       dtype=torch.float16), 'llama.layers.16.ffn_norm.weight': tensor([0.2812, 0.2832, 0.2891,  ..., 0.2812, 0.2832, 0.2734],\n",
      "       dtype=torch.float16), 'llama.layers.17.attention.gate': tensor([[[[ 0.0051]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.0070]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[ 0.0149]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[-0.0043]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[-0.0037]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0077]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         [[ 0.0125]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         [[ 0.0070]]]]), 'llama.layers.17.attention_norm.weight': tensor([0.3965, 0.3945, 0.3828,  ..., 0.3906, 0.3965, 0.3906],\n",
      "       dtype=torch.float16), 'llama.layers.17.ffn_norm.weight': tensor([0.2949, 0.2988, 0.3027,  ..., 0.2930, 0.2988, 0.2910],\n",
      "       dtype=torch.float16), 'llama.layers.18.attention.gate': tensor([[[[ 0.0002]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         [[ 0.0119]],\n",
      "\n",
      "         [[ 0.0045]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         [[ 0.0092]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[ 0.0105]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0169]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[ 0.0114]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[ 0.0159]],\n",
      "\n",
      "         [[ 0.0090]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[ 0.0018]]]]), 'llama.layers.18.attention_norm.weight': tensor([0.3984, 0.4004, 0.3867,  ..., 0.4082, 0.4082, 0.4102],\n",
      "       dtype=torch.float16), 'llama.layers.18.ffn_norm.weight': tensor([0.3184, 0.3184, 0.3242,  ..., 0.3086, 0.3203, 0.3105],\n",
      "       dtype=torch.float16), 'llama.layers.19.attention.gate': tensor([[[[ 0.0005]],\n",
      "\n",
      "         [[ 0.0102]],\n",
      "\n",
      "         [[ 0.0076]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0167]],\n",
      "\n",
      "         [[-0.0059]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[ 0.0065]],\n",
      "\n",
      "         [[-0.0085]],\n",
      "\n",
      "         [[-0.0077]],\n",
      "\n",
      "         [[ 0.0067]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0110]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[ 0.0102]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         [[ 0.0105]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[ 0.0116]],\n",
      "\n",
      "         [[ 0.0109]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[ 0.0064]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0137]],\n",
      "\n",
      "         [[-0.0021]]]]), 'llama.layers.19.attention_norm.weight': tensor([0.3906, 0.3789, 0.3848,  ..., 0.3926, 0.4023, 0.4062],\n",
      "       dtype=torch.float16), 'llama.layers.19.ffn_norm.weight': tensor([0.3301, 0.3320, 0.3340,  ..., 0.3203, 0.3320, 0.3242],\n",
      "       dtype=torch.float16), 'llama.layers.20.attention.gate': tensor([[[[ 0.0059]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0085]],\n",
      "\n",
      "         [[-0.0043]],\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[ 0.0017]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[ 0.0003]]]]), 'llama.layers.20.attention_norm.weight': tensor([0.4102, 0.4160, 0.4023,  ..., 0.4121, 0.4219, 0.4277],\n",
      "       dtype=torch.float16), 'llama.layers.20.ffn_norm.weight': tensor([0.3418, 0.3457, 0.3438,  ..., 0.3320, 0.3438, 0.3457],\n",
      "       dtype=torch.float16), 'llama.layers.21.attention.gate': tensor([[[[ 0.0082]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         [[-0.0172]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0110]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[ 0.0079]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[ 0.0103]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[ 0.0077]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[ 0.0144]],\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0050]]]]), 'llama.layers.21.attention_norm.weight': tensor([0.4199, 0.4160, 0.4199,  ..., 0.4199, 0.4180, 0.4316],\n",
      "       dtype=torch.float16), 'llama.layers.21.ffn_norm.weight': tensor([0.3555, 0.3594, 0.3633,  ..., 0.3555, 0.3633, 0.3574],\n",
      "       dtype=torch.float16), 'llama.layers.22.attention.gate': tensor([[[[-0.0071]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         [[-0.0121]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         [[-0.0128]],\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[-0.0090]],\n",
      "\n",
      "         [[-0.0060]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[ 0.0134]],\n",
      "\n",
      "         [[-0.0064]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0108]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[ 0.0066]]]]), 'llama.layers.22.attention_norm.weight': tensor([0.4199, 0.4336, 0.4316,  ..., 0.4375, 0.4453, 0.4492],\n",
      "       dtype=torch.float16), 'llama.layers.22.ffn_norm.weight': tensor([0.3770, 0.3789, 0.3770,  ..., 0.3730, 0.3809, 0.3750],\n",
      "       dtype=torch.float16), 'llama.layers.23.attention.gate': tensor([[[[-0.0003]],\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[-0.0076]],\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0050]],\n",
      "\n",
      "         [[ 0.0088]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[ 0.0082]],\n",
      "\n",
      "         [[ 0.0085]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[-0.0029]],\n",
      "\n",
      "         [[ 0.0209]],\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[-0.0098]],\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[ 0.0122]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         [[ 0.0056]],\n",
      "\n",
      "         [[-0.0060]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[ 0.0251]],\n",
      "\n",
      "         [[ 0.0069]]]]), 'llama.layers.23.attention_norm.weight': tensor([0.4375, 0.4395, 0.4355,  ..., 0.4355, 0.4395, 0.4492],\n",
      "       dtype=torch.float16), 'llama.layers.23.ffn_norm.weight': tensor([0.3926, 0.3965, 0.3926,  ..., 0.3809, 0.4043, 0.3828],\n",
      "       dtype=torch.float16), 'llama.layers.24.attention.gate': tensor([[[[ 0.0079]],\n",
      "\n",
      "         [[ 0.0112]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[-0.0104]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0104]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[ 0.0229]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[-0.0043]],\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[ 0.0189]],\n",
      "\n",
      "         [[-0.0050]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[-0.0156]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0182]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[-0.0088]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0017]]]]), 'llama.layers.24.attention_norm.weight': tensor([0.4570, 0.4727, 0.4668,  ..., 0.4727, 0.4727, 0.4824],\n",
      "       dtype=torch.float16), 'llama.layers.24.ffn_norm.weight': tensor([0.4043, 0.4082, 0.4062,  ..., 0.4023, 0.4043, 0.4062],\n",
      "       dtype=torch.float16), 'llama.layers.25.attention.gate': tensor([[[[ 0.0002]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[-0.0090]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         [[-0.0211]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0117]],\n",
      "\n",
      "         [[ 0.0017]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[ 0.0109]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         [[ 0.0295]],\n",
      "\n",
      "         [[-0.0104]],\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0129]],\n",
      "\n",
      "         [[-0.0051]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[-0.0141]],\n",
      "\n",
      "         [[ 0.0100]],\n",
      "\n",
      "         [[-0.0131]],\n",
      "\n",
      "         [[-0.0077]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         [[ 0.0064]],\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         [[-0.0023]]]]), 'llama.layers.25.attention_norm.weight': tensor([0.4883, 0.4824, 0.4766,  ..., 0.5000, 0.4902, 0.4883],\n",
      "       dtype=torch.float16), 'llama.layers.25.ffn_norm.weight': tensor([0.4141, 0.4160, 0.4219,  ..., 0.4121, 0.4258, 0.4180],\n",
      "       dtype=torch.float16), 'llama.layers.26.attention.gate': tensor([[[[-0.0074]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0121]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[ 0.0151]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[-0.0111]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         [[-0.0071]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[ 0.0127]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         [[-0.0122]],\n",
      "\n",
      "         [[ 0.0243]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0154]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0186]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[-0.0060]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         [[-0.0217]]]]), 'llama.layers.26.attention_norm.weight': tensor([0.5039, 0.5117, 0.5156,  ..., 0.5234, 0.5234, 0.5156],\n",
      "       dtype=torch.float16), 'llama.layers.26.ffn_norm.weight': tensor([0.4297, 0.4316, 0.4297,  ..., 0.4238, 0.4355, 0.4336],\n",
      "       dtype=torch.float16), 'llama.layers.27.attention.gate': tensor([[[[-0.0112]],\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0147]],\n",
      "\n",
      "         [[-0.0102]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[-0.0076]],\n",
      "\n",
      "         [[-0.0209]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[-0.0202]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[-0.0236]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[ 0.0085]],\n",
      "\n",
      "         [[-0.0122]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         [[-0.0186]],\n",
      "\n",
      "         [[-0.0132]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0031]]]]), 'llama.layers.27.attention_norm.weight': tensor([0.4707, 0.5234, 0.5234,  ..., 0.5234, 0.5391, 0.5156],\n",
      "       dtype=torch.float16), 'llama.layers.27.ffn_norm.weight': tensor([0.4512, 0.4453, 0.4453,  ..., 0.4414, 0.4531, 0.4414],\n",
      "       dtype=torch.float16), 'llama.layers.28.attention.gate': tensor([[[[ 1.1060e-02]],\n",
      "\n",
      "         [[ 1.4682e-02]],\n",
      "\n",
      "         [[ 1.9674e-03]],\n",
      "\n",
      "         [[-2.5451e-03]],\n",
      "\n",
      "         [[-1.5241e-02]],\n",
      "\n",
      "         [[ 3.3478e-03]],\n",
      "\n",
      "         [[ 7.3606e-03]],\n",
      "\n",
      "         [[-4.3351e-03]],\n",
      "\n",
      "         [[ 5.9227e-03]],\n",
      "\n",
      "         [[-2.9612e-03]],\n",
      "\n",
      "         [[ 3.4169e-02]],\n",
      "\n",
      "         [[-1.3492e-02]],\n",
      "\n",
      "         [[-8.2334e-03]],\n",
      "\n",
      "         [[ 7.1789e-04]],\n",
      "\n",
      "         [[ 3.5040e-03]],\n",
      "\n",
      "         [[ 6.0457e-03]],\n",
      "\n",
      "         [[-1.7240e-02]],\n",
      "\n",
      "         [[ 8.7539e-03]],\n",
      "\n",
      "         [[-8.4685e-03]],\n",
      "\n",
      "         [[ 2.3166e-02]],\n",
      "\n",
      "         [[ 2.8746e-02]],\n",
      "\n",
      "         [[ 2.5468e-02]],\n",
      "\n",
      "         [[-1.2658e-02]],\n",
      "\n",
      "         [[ 2.3564e-05]],\n",
      "\n",
      "         [[-1.5436e-03]],\n",
      "\n",
      "         [[-4.3237e-02]],\n",
      "\n",
      "         [[ 4.0361e-03]],\n",
      "\n",
      "         [[-1.0214e-02]],\n",
      "\n",
      "         [[-6.9366e-03]],\n",
      "\n",
      "         [[-4.2725e-04]],\n",
      "\n",
      "         [[-2.2933e-04]],\n",
      "\n",
      "         [[ 8.6366e-03]]]]), 'llama.layers.28.attention_norm.weight': tensor([0.4707, 0.5156, 0.4961,  ..., 0.5039, 0.5195, 0.5156],\n",
      "       dtype=torch.float16), 'llama.layers.28.ffn_norm.weight': tensor([0.4590, 0.4531, 0.4668,  ..., 0.4492, 0.4688, 0.4531],\n",
      "       dtype=torch.float16), 'llama.layers.29.attention.gate': tensor([[[[-0.0274]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0198]],\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0154]],\n",
      "\n",
      "         [[ 0.0138]],\n",
      "\n",
      "         [[-0.0091]],\n",
      "\n",
      "         [[ 0.0309]],\n",
      "\n",
      "         [[-0.0388]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[-0.0318]],\n",
      "\n",
      "         [[-0.0225]],\n",
      "\n",
      "         [[-0.0142]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[-0.0097]],\n",
      "\n",
      "         [[ 0.0056]],\n",
      "\n",
      "         [[ 0.0063]],\n",
      "\n",
      "         [[-0.0123]],\n",
      "\n",
      "         [[ 0.0065]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         [[-0.0239]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         [[ 0.0131]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0089]],\n",
      "\n",
      "         [[ 0.0086]]]]), 'llama.layers.29.attention_norm.weight': tensor([0.4824, 0.5234, 0.5156,  ..., 0.5234, 0.5391, 0.5312],\n",
      "       dtype=torch.float16), 'llama.layers.29.ffn_norm.weight': tensor([0.4805, 0.4746, 0.4668,  ..., 0.4551, 0.4746, 0.4648],\n",
      "       dtype=torch.float16), 'llama.layers.30.attention.gate': tensor([[[[ 0.0134]],\n",
      "\n",
      "         [[-0.0104]],\n",
      "\n",
      "         [[-0.0114]],\n",
      "\n",
      "         [[-0.0105]],\n",
      "\n",
      "         [[ 0.0489]],\n",
      "\n",
      "         [[-0.0267]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[-0.0222]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[-0.0252]],\n",
      "\n",
      "         [[-0.0163]],\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         [[-0.0064]],\n",
      "\n",
      "         [[-0.0159]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[-0.0029]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[-0.0285]],\n",
      "\n",
      "         [[ 0.0008]],\n",
      "\n",
      "         [[ 0.0052]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         [[ 0.0144]],\n",
      "\n",
      "         [[ 0.0080]]]]), 'llama.layers.30.attention_norm.weight': tensor([0.4766, 0.5156, 0.5117,  ..., 0.5195, 0.5352, 0.5156],\n",
      "       dtype=torch.float16), 'llama.layers.30.ffn_norm.weight': tensor([0.4648, 0.4688, 0.4668,  ..., 0.4570, 0.4668, 0.4570],\n",
      "       dtype=torch.float16), 'llama.layers.31.attention.gate': tensor([[[[-0.0102]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0091]],\n",
      "\n",
      "         [[ 0.0660]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[-0.0231]],\n",
      "\n",
      "         [[ 0.0637]],\n",
      "\n",
      "         [[-0.0132]],\n",
      "\n",
      "         [[-0.0105]],\n",
      "\n",
      "         [[-0.0425]],\n",
      "\n",
      "         [[ 0.0404]],\n",
      "\n",
      "         [[ 0.0100]],\n",
      "\n",
      "         [[-0.0257]],\n",
      "\n",
      "         [[-0.0064]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0219]],\n",
      "\n",
      "         [[-0.0424]],\n",
      "\n",
      "         [[-0.0251]],\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         [[ 0.0321]],\n",
      "\n",
      "         [[-0.0185]],\n",
      "\n",
      "         [[ 0.0144]],\n",
      "\n",
      "         [[ 0.0058]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0228]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[-0.0290]],\n",
      "\n",
      "         [[-0.0217]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         [[-0.0327]]]]), 'llama.layers.31.attention_norm.weight': tensor([0.3359, 0.4219, 0.4316,  ..., 0.4160, 0.4102, 0.4316],\n",
      "       dtype=torch.float16), 'llama.layers.31.ffn_norm.weight': tensor([0.4043, 0.4062, 0.4297,  ..., 0.4121, 0.4258, 0.4141],\n",
      "       dtype=torch.float16), 'llama.norm.weight': tensor([1.9219, 1.6328, 1.6875,  ..., 1.7500, 1.7266, 1.6172],\n",
      "       dtype=torch.float16), 'prefix_query.weight': tensor([[ 0.8304, -1.4537, -0.9831,  ...,  0.5564,  0.4487,  0.2802],\n",
      "        [-1.3521,  0.2607, -0.6985,  ..., -0.6863,  1.8258,  1.8429],\n",
      "        [-2.6016,  0.4872, -0.7063,  ..., -0.4777,  0.1033, -0.2875],\n",
      "        ...,\n",
      "        [ 0.2339, -0.7778, -0.4420,  ...,  1.1496,  1.2496,  1.2033],\n",
      "        [-0.3505, -1.0480,  0.9379,  ...,  1.3831,  0.4351, -1.2557],\n",
      "        [-0.4410,  0.5410,  1.8467,  ...,  1.4086,  0.9128, -1.1141]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pth_file_path = r\"C:\\Users\\r-cet\\Downloads\\7B-pretrained.pth\"\n",
    "loaded_data = torch.load(pth_file_path, map_location=\"cpu\")\n",
    "print(\"loaded data keys:\", loaded_data.keys())\n",
    "for key, value in loaded_data.items():\n",
    "    print(f\"Key: {key}, Type: {type(value)}, Shape: {value.shape if isinstance(value, torch.Tensor) else 'Not a Tensor'}\")\n",
    "for key, value in loaded_data.items():\n",
    "    print(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc18dcd-8426-4590-b52e-ad1df9663812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
